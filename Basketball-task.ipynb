{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "90fS1F3Uffmz"
      },
      "source": [
        ".\n",
        "..............................................................................................................................................................................................................................## **Preprocessing** **part**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TZzWpHekWBT1"
      },
      "source": [
        "Install necessary function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Y_cnOuyWA_o",
        "outputId": "8b55f7e0-faef-4ca4-ad74-0ea40a8002a2"
      },
      "outputs": [],
      "source": [
        "!pip install torch torchvision\n",
        "!pip install opencv-python\n",
        "\n",
        "import json\n",
        "import numpy as np\n",
        "import cv2\n",
        "import torch\n",
        "import os\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import transforms\n",
        "from google.colab import drive\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-dd-io8W7lov",
        "outputId": "fdf55b09-77fa-4554-de35-42f7e8175ed3"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CyyrAfZidAzy"
      },
      "source": [
        "Augmentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JYcD-hp9dAgH",
        "outputId": "b0328a20-2c01-498a-cad9-bc0460c4dd1f"
      },
      "outputs": [],
      "source": [
        "base_dir = '/content/drive/My Drive/dataset/'\n",
        "output_dir = base_dir + 'augmented-examples/'\n",
        "if not os.path.exists(output_dir):\n",
        "    os.makedirs(output_dir)\n",
        "def keystoint(x):\n",
        "    \"\"\" Convert dictionary keys to integers, useful for JSON keys. \"\"\"\n",
        "    return {int(k): v for k, v in x.items()}\n",
        "\n",
        "def rotateVideo(path, output_dir, video_id, degree):\n",
        "    \"\"\" Rotate the video by a specified degree and save it. \"\"\"\n",
        "\n",
        "    video = cv2.VideoCapture(path)\n",
        "    frame_width = int(video.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "    frame_height = int(video.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "    fps = video.get(cv2.CAP_PROP_FPS)\n",
        "\n",
        "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "    out_rotate = cv2.VideoWriter(output_dir + video_id + \"_rotate_\" + str(degree) + \".mp4\", fourcc, fps, (frame_width, frame_height))\n",
        "\n",
        "    while video.isOpened():\n",
        "        success, frame = video.read()\n",
        "        if not success:\n",
        "            break\n",
        "\n",
        "        rotation_matrix = cv2.getRotationMatrix2D((frame_width / 2, frame_height / 2), degree, 1)\n",
        "        rotated_frame = cv2.warpAffine(frame, rotation_matrix, (frame_width, frame_height))\n",
        "        out_rotate.write(rotated_frame)\n",
        "\n",
        "    video.release()\n",
        "    out_rotate.release()\n",
        "\n",
        "def translateVideo(path, output_dir, video_id, translate=(0, 0)):\n",
        "    \"\"\" Translate the video by specified pixels and save it. \"\"\"\n",
        "    video = cv2.VideoCapture(path)\n",
        "    frame_width = int(video.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "    frame_height = int(video.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "    fps = video.get(cv2.CAP_PROP_FPS)\n",
        "\n",
        "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "    out_translate = cv2.VideoWriter(output_dir + video_id + \"_translate_\" + str(translate[0]) + \"_\" + str(translate[1]) + \".mp4\", fourcc, fps, (frame_width, frame_height))\n",
        "\n",
        "    while video.isOpened():\n",
        "        success, frame = video.read()\n",
        "        if not success:\n",
        "            break\n",
        "\n",
        "        M = np.float32([[1, 0, translate[0]], [0, 1, translate[1]]])\n",
        "        dst = cv2.warpAffine(frame, M, (frame_width, frame_height))\n",
        "        out_translate.write(dst)\n",
        "\n",
        "    video.release()\n",
        "    out_translate.release()\n",
        "\n",
        "def augmentVideo(annotation_dict, labels_dict, data_dir=base_dir + 'examples/', output_dir=base_dir + 'augmented-examples/'):\n",
        "    \"\"\" Augment videos with rotation and translation transformations. \"\"\"\n",
        "    with open(annotation_dict, 'r') as f:\n",
        "        annotation_dict = json.load(f)\n",
        "        video_list = list(annotation_dict.items())\n",
        "\n",
        "    with open(labels_dict, 'r') as f:\n",
        "        labels_dict = json.load(f,object_hook=keystoint)\n",
        "\n",
        "    # Count the distribution of actions\n",
        "    count_dict = dict()\n",
        "    for key, value in annotation_dict.items():\n",
        "        action = labels_dict[value]\n",
        "        if action in count_dict:\n",
        "            count_dict[action] += 1\n",
        "        else:\n",
        "            count_dict[action] = 1\n",
        "\n",
        "    # Find actions with fewer than 2000 examples\n",
        "    filtered_actions = [action for action, count in count_dict.items() if count <= 2000]\n",
        "\n",
        "    # Prepare to augment videos\n",
        "    augmented_annotation = dict()\n",
        "    pbar = tqdm(video_list)\n",
        "    for video_id, action in pbar:\n",
        "        if labels_dict[action] in filtered_actions:\n",
        "            path = data_dir + video_id + \".mp4\"\n",
        "\n",
        "            # Rotate and translate the video, save augmented videos\n",
        "            augmented_annotation[video_id + \"_rotate_30\"] = action\n",
        "            rotateVideo(path, output_dir, video_id, 30)\n",
        "\n",
        "            augmented_annotation[video_id + \"_rotate_330\"] = action\n",
        "            rotateVideo(path, output_dir, video_id, 330)\n",
        "\n",
        "            augmented_annotation[video_id + \"_translate_32_0\"] = action\n",
        "            translateVideo(path, output_dir, video_id, (32, 0))\n",
        "\n",
        "            augmented_annotation[video_id + \"_translate_-32_0\"] = action\n",
        "            translateVideo(path, output_dir, video_id, (-32, 0))\n",
        "\n",
        "    # Save new annotations\n",
        "    with open(output_dir + 'augmented_annotation_dict.json', 'w') as fp:\n",
        "        json.dump(augmented_annotation, fp)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    augmentVideo(base_dir + \"annotation_dict.json\", base_dir + \"labels_dict.json\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "btbjgshUTrY3"
      },
      "source": [
        "Loading data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jMaQlQFR2Uu1",
        "outputId": "da6465e0-a84a-4c01-b84b-b2e7788ef625"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total samples: 37085\n",
            "Class counts:\n",
            "block: 996\n",
            "walk: 11749\n",
            "defense: 3866\n",
            "pass: 1070\n",
            "no_action: 6490\n",
            "run: 5924\n",
            "shoot: 426\n",
            "dribble: 3490\n",
            "pick: 712\n",
            "ball in hand: 2362\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "\n",
        "def load_dict_from_json(file_path):\n",
        "    with open(file_path) as f:\n",
        "        return json.load(f)\n",
        "\n",
        "def count_samples(annotation_dict_path, label_dict_path):\n",
        "    annotation_dict = load_dict_from_json(annotation_dict_path)\n",
        "    label_dict = load_dict_from_json(label_dict_path)\n",
        "\n",
        "    total_samples = len(annotation_dict)\n",
        "    class_counts = {label: 0 for label in set(label_dict.values()) if label != \"discard\"}\n",
        "\n",
        "    for video_id, label_index in annotation_dict.items():\n",
        "        label = label_dict[str(label_index)]\n",
        "        if label != \"discard\":\n",
        "            class_counts[label] += 1\n",
        "\n",
        "    return total_samples, class_counts\n",
        "\n",
        "# 設置 annotation_dict 和 label_dict 的路徑\n",
        "annotation_dict_path = '/content/drive/My Drive/dataset/annotation_dict.json'\n",
        "label_dict_path = '/content/drive/My Drive/dataset/labels_dict.json'\n",
        "\n",
        "# 計算總樣本數和各個類別的數量\n",
        "total_samples, class_counts = count_samples(annotation_dict_path, label_dict_path)\n",
        "\n",
        "print(\"Total samples:\", total_samples)\n",
        "print(\"Class counts:\")\n",
        "for label, count in class_counts.items():\n",
        "    print(f\"{label}: {count}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sG7lgEIBz7u1",
        "outputId": "61dac185-2ce6-48cf-a151-7380a2f5e2ae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total samples: 12814\n",
            "Class counts:\n",
            "block: 3982\n",
            "walk: 0\n",
            "defense: 0\n",
            "pass: 4280\n",
            "no_action: 0\n",
            "run: 0\n",
            "shoot: 1704\n",
            "dribble: 0\n",
            "pick: 2848\n",
            "ball in hand: 0\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "\n",
        "def load_dict_from_json(file_path):\n",
        "    with open(file_path) as f:\n",
        "        return json.load(f)\n",
        "\n",
        "def count_samples(annotation_dict_path, label_dict_path):\n",
        "    annotation_dict = load_dict_from_json(annotation_dict_path)\n",
        "    label_dict = load_dict_from_json(label_dict_path)\n",
        "\n",
        "    total_samples = len(annotation_dict)\n",
        "    class_counts = {label: 0 for label in set(label_dict.values()) if label != \"discard\"}\n",
        "\n",
        "    for video_id, label_index in annotation_dict.items():\n",
        "        label = label_dict[str(label_index)]\n",
        "        if label != \"discard\":\n",
        "            class_counts[label] += 1\n",
        "\n",
        "    return total_samples, class_counts\n",
        "\n",
        "# 設置 annotation_dict 和 label_dict 的路徑\n",
        "annotation_dict_path = '/content/drive/My Drive/dataset/augmented_annotation_dict.json'\n",
        "label_dict_path = '/content/drive/My Drive/dataset/labels_dict.json'\n",
        "\n",
        "# 計算總樣本數和各個類別的數量\n",
        "total_samples, class_counts = count_samples(annotation_dict_path, label_dict_path)\n",
        "\n",
        "print(\"Total samples:\", total_samples)\n",
        "print(\"Class counts:\")\n",
        "for label, count in class_counts.items():\n",
        "    print(f\"{label}: {count}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gIIZMhckqiTb",
        "outputId": "e6393a89-c433-4cd5-ae55-542eb4d2d85b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0.], dtype=torch.float64)\n",
            "8\n",
            "49899\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import numpy as np\n",
        "import cv2\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import transforms\n",
        "base_dir = '/content/drive/My Drive/dataset/'\n",
        "class BasketballDataset(Dataset):\n",
        "    \"\"\"SpaceJam: a Dataset for Basketball Action Recognition.\"\"\"\n",
        "\n",
        "    def __init__(self, annotation_dict, augmented_dict, video_dir=base_dir+\"examples/\", augmented_dir=base_dir+\"augmented-examples/\", augment=True, transform=None, poseData=False):\n",
        "        with open(annotation_dict) as f:\n",
        "            self.video_list = list(json.load(f).items())\n",
        "\n",
        "        if augment == True:\n",
        "            self.augment = augment\n",
        "            with open(augmented_dict) as f:\n",
        "                augmented_list = list(json.load(f).items())\n",
        "            self.augmented_dir = augmented_dir\n",
        "            # extend with augmented data\n",
        "            self.video_list.extend(augmented_list)\n",
        "\n",
        "        self.video_dir = video_dir\n",
        "        self.poseData = poseData\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        # return length of none-flipped videos in directory\n",
        "        return len(self.video_list)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        video_id = self.video_list[idx][0]\n",
        "        encoding = np.squeeze(np.eye(10)[np.array([0,1,2,3,4,5,6,7,8,9]).reshape(-1)])\n",
        "        if self.poseData and self.augment==False:\n",
        "            joints = np.load(self.video_dir + video_id + \".npy\", allow_pickle=True)\n",
        "            sample = {'video_id': video_id, 'joints': joints, 'action': torch.from_numpy(np.array(encoding[self.video_list[idx][1]])), 'class': self.video_list[idx][1]}\n",
        "        else:\n",
        "            video = self.VideoToNumpy(video_id)\n",
        "            sample = {'video_id': video_id, 'video': torch.from_numpy(video).float(), 'action': torch.from_numpy(np.array(encoding[self.video_list[idx][1]])), 'class': self.video_list[idx][1]}\n",
        "\n",
        "        return sample\n",
        "\n",
        "    def keystoint(self, x):\n",
        "        return {int(k): v for k, v in x.items()}\n",
        "\n",
        "    def VideoToNumpy(self, video_id):\n",
        "        # get video\n",
        "        video = cv2.VideoCapture(self.video_dir + video_id + \".mp4\")\n",
        "\n",
        "        if not video.isOpened():\n",
        "            video = cv2.VideoCapture(self.augmented_dir + video_id + \".mp4\")\n",
        "        if not video.isOpened():\n",
        "            raise Exception(\"Video file not readable\")\n",
        "\n",
        "        video_frames = []\n",
        "        while (video.isOpened()):\n",
        "            # read video\n",
        "            success, frame = video.read()\n",
        "            if not success:\n",
        "                break\n",
        "\n",
        "            frame = np.asarray([frame[..., i] for i in range(frame.shape[-1])]).astype(float)\n",
        "            video_frames.append(frame)\n",
        "\n",
        "        video.release()\n",
        "        assert len(video_frames) == 16\n",
        "        return np.transpose(np.asarray(video_frames), (1,0,2,3))\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    basketball_dataset = BasketballDataset(annotation_dict= base_dir +\"annotation_dict.json\",\n",
        "                                           augmented_dict= base_dir+\"augmented_annotation_dict.json\")\n",
        "\n",
        "    print(basketball_dataset[1]['action'])\n",
        "    print(basketball_dataset[1]['class'])\n",
        "    print(len(basketball_dataset))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BB40koX8fXWH"
      },
      "source": [
        "## **Model**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Mxvd58--VJl"
      },
      "source": [
        "Majority Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NX0IKF7_fXH_",
        "outputId": "c084e17c-0967-4f5d-ac43-c7751db3da2f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The majority class is '9' with 11749 instances.\n",
            "Baseline accuracy using the majority class is 23.55%.\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "\n",
        "def load_annotations(paths):\n",
        "    \"\"\" Load and combine annotation data from multiple JSON files. \"\"\"\n",
        "    combined_annotations = {}\n",
        "    for path in paths:\n",
        "        with open(path, 'r') as file:\n",
        "            annotations = json.load(file)\n",
        "            combined_annotations.update(annotations)\n",
        "    return combined_annotations\n",
        "\n",
        "def find_majority_class(annotations):\n",
        "    \"\"\" Find the majority class in the annotation data. \"\"\"\n",
        "    class_count = {}\n",
        "    for action in annotations.values():\n",
        "        if action in class_count:\n",
        "            class_count[action] += 1\n",
        "        else:\n",
        "            class_count[action] = 1\n",
        "    majority_class = max(class_count, key=class_count.get)\n",
        "    return majority_class, class_count[majority_class]\n",
        "\n",
        "def evaluate_baseline(majority_class, annotations):\n",
        "    \"\"\" Evaluate the baseline model based on the majority class. \"\"\"\n",
        "    correct_predictions = sum(1 for action in annotations.values() if action == majority_class)\n",
        "    total_predictions = len(annotations)\n",
        "    accuracy = correct_predictions / total_predictions\n",
        "    return accuracy\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Define the paths to your annotation files\n",
        "    annotation_paths = [\n",
        "        '/content/drive/My Drive/dataset/annotation_dict.json',\n",
        "        '/content/drive/My Drive/dataset/augmented_annotation_dict.json'\n",
        "    ]\n",
        "\n",
        "    # Load and combine annotations from both files\n",
        "    annotations = load_annotations(annotation_paths)\n",
        "\n",
        "    # Find the majority class\n",
        "    majority_class, count = find_majority_class(annotations)\n",
        "    print(f\"The majority class is '{majority_class}' with {count} instances.\")\n",
        "\n",
        "    # Evaluate the baseline model\n",
        "    accuracy = evaluate_baseline(majority_class, annotations)\n",
        "    print(f\"Baseline accuracy using the majority class is {accuracy:.2%}.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w8OQbSX6dvMA",
        "outputId": "7079e156-86f2-4160-eeea-a1df3a27c895"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The majority class is '9' with 11749 instances.\n",
            "Baseline precision for the majority class is 23.55%.\n",
            "Baseline recall for the majority class is 100.00%.\n",
            "Baseline F1 score for the majority class is 38.12%.\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "from sklearn.metrics import f1_score, precision_score, recall_score\n",
        "\n",
        "def load_annotations(paths):\n",
        "    \"\"\" Load and combine annotation data from multiple JSON files. \"\"\"\n",
        "    combined_annotations = {}\n",
        "    for path in paths:\n",
        "        with open(path, 'r') as file:\n",
        "            annotations = json.load(file)\n",
        "            combined_annotations.update(annotations)\n",
        "    return combined_annotations\n",
        "\n",
        "def find_majority_class(annotations):\n",
        "    \"\"\" Find the majority class in the annotation data. \"\"\"\n",
        "    class_count = {}\n",
        "    for action in annotations.values():\n",
        "        if action in class_count:\n",
        "            class_count[action] += 1\n",
        "        else:\n",
        "            class_count[action] = 1\n",
        "    majority_class = max(class_count, key=class_count.get)\n",
        "    return majority_class, class_count[majority_class]\n",
        "\n",
        "def evaluate_baseline(majority_class, annotations):\n",
        "    \"\"\" Evaluate the baseline model based on the majority class. \"\"\"\n",
        "    y_true = list(annotations.values())\n",
        "    y_pred = [majority_class] * len(y_true)  # Predicting majority class for all instances\n",
        "\n",
        "    # Calculating precision, recall, and F1 score\n",
        "    precision = precision_score(y_true, y_pred, average='macro', labels=[majority_class])\n",
        "    recall = recall_score(y_true, y_pred, average='macro', labels=[majority_class])\n",
        "    f1 = f1_score(y_true, y_pred, average='macro', labels=[majority_class])\n",
        "\n",
        "    return precision, recall, f1\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Define the paths to your annotation files\n",
        "    annotation_paths = [\n",
        "        '/content/drive/My Drive/dataset/annotation_dict.json',\n",
        "        '/content/drive/My Drive/dataset/augmented_annotation_dict.json'\n",
        "    ]\n",
        "\n",
        "    # Load and combine annotations from both files\n",
        "    annotations = load_annotations(annotation_paths)\n",
        "\n",
        "    # Find the majority class\n",
        "    majority_class, count = find_majority_class(annotations)\n",
        "    print(f\"The majority class is '{majority_class}' with {count} instances.\")\n",
        "\n",
        "    # Evaluate the baseline model\n",
        "    precision, recall, f1 = evaluate_baseline(majority_class, annotations)\n",
        "    print(f\"Baseline precision for the majority class is {precision:.2%}.\")\n",
        "    print(f\"Baseline recall for the majority class is {recall:.2%}.\")\n",
        "    print(f\"Baseline F1 score for the majority class is {f1:.2%}.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Af9lLk-My59M"
      },
      "source": [
        "R(2+1)D CNN Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F5YhNKGQJuN6"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import f1_score, precision_score, recall_score\n",
        "\n",
        "def get_acc_f1_precision_recall(pred_classes, ground_truths, labels=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]):\n",
        "    \"\"\"\n",
        "    Given two linear arrays of predicted classes and ground truths, return accuracy, f1 score, precision and recall\n",
        "    :param pred_classes: classes predicted by model\n",
        "    :param ground_truths: ground truths for predictions\n",
        "    :return: tuple of accuracy, f1, precision, recall\n",
        "    \"\"\"\n",
        "\n",
        "    print(pred_classes)\n",
        "    print(ground_truths)\n",
        "\n",
        "    accuracy = np.mean((pred_classes == ground_truths)).astype(np.float64)\n",
        "    f1 = f1_score(ground_truths, pred_classes, labels=labels, average='micro')\n",
        "    precision = precision_score(ground_truths, pred_classes, labels=labels, average='micro')\n",
        "    recall = recall_score(ground_truths, pred_classes, labels=labels, average='micro')\n",
        "\n",
        "    return accuracy, f1, precision, recall"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z_3YmO4bzRyA",
        "outputId": "ec9f8a19-8968-4552-f35b-3379a0dc0421"
      },
      "outputs": [],
      "source": [
        "pip install vidaug\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Bxig6nCZBgLD",
        "outputId": "f79763f1-82ff-470e-8d06-e2377b4f366d"
      },
      "outputs": [],
      "source": [
        "from __future__ import print_function\n",
        "from __future__ import division\n",
        "\n",
        "import numpy as np\n",
        "import copy\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "from easydict import EasyDict\n",
        "from vidaug import augmentors as vidaug\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "import torch\n",
        "from torch.nn import Dropout\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "from torchvision import models\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import sys\n",
        "\n",
        "# 确保替换为您的utils文件夹所在的父目录的完整路径\n",
        "sys.path.append('/content/drive/My Drive/dataset')\n",
        "\n",
        "# 现在尝试导入模块\n",
        "from utils.checkpoints import init_session_history, save_weights, load_weights, write_history\n",
        "\n",
        "# 将CUDA设备设置为默认设备（如果可用）\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "args = EasyDict({\n",
        "    'base_model_name': 'r2plus1d_multiclass',\n",
        "    'pretrained': True,\n",
        "\n",
        "    # training/model params\n",
        "    'lr': 0.0001,\n",
        "    'start_epoch': 1,\n",
        "    'num_epochs': 6,\n",
        "    'layers_list': ['layer3', 'layer4', 'fc'],\n",
        "    'continue_epoch': False,\n",
        "    'device': 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    # Dataset params\n",
        "    'num_classes': 10,\n",
        "    'batch_size': 25,\n",
        "    'n_total': 49899,\n",
        "    'test_n': 4990,\n",
        "    'val_n': 9980,\n",
        "\n",
        "    # Path params\n",
        "    'annotation_path': base_dir+\"annotation_dict.json\",\n",
        "    'augmented_annotation_path':base_dir+\"augmented_annotation_dict.json\",\n",
        "    'model_path': base_dir+\"model-checkpoints/\",\n",
        "    'history_path': base_dir+\"histories/history3.txt\"\n",
        "})\n",
        "\n",
        "def train_model(model, dataloaders, criterion, optimizer, args, start_epoch=1, num_epochs=25):\n",
        "    \"\"\"\n",
        "    Trains the 3D CNN Model\n",
        "    :param model: Model object that we will train\n",
        "    :param base_model_name: The base name of the model\n",
        "    :param dataloaders: A dictionary of train and validation dataloader\n",
        "    :param criterion: Pytorch Criterion Instance\n",
        "    :param optimizer: Pytorch Optimizer Instance\n",
        "    :param num_epochs: Number of epochs during training\n",
        "    :return: model, train_loss_history, val_loss_history, train_acc_history, val_acc_history, train_f1_score, val_f1_score, plot_epoch\n",
        "    \"\"\"\n",
        "\n",
        "    # Initializes Session History in the history file\n",
        "    init_session_history(args)\n",
        "    since = time.time()\n",
        "\n",
        "    train_acc_history = []\n",
        "    val_acc_history = []\n",
        "    train_loss_history = []\n",
        "    val_loss_history = []\n",
        "    train_f1_score = []\n",
        "    val_f1_score = []\n",
        "    plot_epoch = []\n",
        "\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_acc = 0.0\n",
        "\n",
        "    for epoch in range(start_epoch, num_epochs):\n",
        "\n",
        "        # Each epoch has a training and validation phase\n",
        "        for phase in ['train', 'val']:\n",
        "            if phase == 'train':\n",
        "                model.train()  # Set model to training mode\n",
        "                train_pred_classes = []\n",
        "                train_ground_truths = []\n",
        "            else:\n",
        "                model.eval()  # Set model to evaluate mode\n",
        "                val_pred_classes = []\n",
        "                val_ground_truths = []\n",
        "\n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0\n",
        "            train_n_total = 1\n",
        "\n",
        "            pbar = tqdm(dataloaders[phase])\n",
        "            # Iterate over data.\n",
        "            for sample in pbar:\n",
        "                inputs = sample[\"video\"].to(device)\n",
        "                labels = sample[\"action\"].to(device)\n",
        "\n",
        "                # zero the parameter gradients\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # forward\n",
        "                # track history if only in train\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    outputs = model(inputs)\n",
        "                    loss = criterion(outputs, torch.max(labels, 1)[1])\n",
        "\n",
        "                    _, preds = torch.max(outputs, 1)\n",
        "\n",
        "                    if phase == 'train':\n",
        "                        train_pred_classes.extend(preds.detach().cpu().numpy())\n",
        "                        train_ground_truths.extend(torch.max(labels, 1)[1].detach().cpu().numpy())\n",
        "                    else:\n",
        "                        val_pred_classes.extend(preds.detach().cpu().numpy())\n",
        "                        val_ground_truths.extend(torch.max(labels, 1)[1].detach().cpu().numpy())\n",
        "\n",
        "                    # backward + optimize only if in training phase\n",
        "                    if phase == 'train':\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                # statistics\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                running_corrects += torch.sum(preds == torch.max(labels, 1)[1])\n",
        "\n",
        "                pbar.set_description('Phase: {} || Epoch: {} || Loss {:.5f} '.format(phase, epoch, running_loss / train_n_total))\n",
        "                train_n_total += 1\n",
        "\n",
        "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
        "            epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n",
        "\n",
        "            # Calculate elapsed time\n",
        "            time_elapsed = time.time() - since\n",
        "            print(phase, ' training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
        "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
        "\n",
        "            # For Checkpointing and Confusion Matrix\n",
        "            if phase == 'val':\n",
        "                val_acc_history.append(epoch_acc)\n",
        "                val_loss_history.append(epoch_loss)\n",
        "                val_pred_classes = np.asarray(val_pred_classes)\n",
        "                val_ground_truths = np.asarray(val_ground_truths)\n",
        "                val_accuracy, val_f1, val_precision, val_recall = get_acc_f1_precision_recall(\n",
        "                    val_pred_classes, val_ground_truths\n",
        "                )\n",
        "                val_f1_score.append(val_f1)\n",
        "                val_confusion_matrix = np.array_str(confusion_matrix(val_ground_truths, val_pred_classes, labels=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]))\n",
        "                print('Epoch: {} || Val_Acc: {} || Val_Loss: {}'.format(\n",
        "                    epoch, val_accuracy, epoch_loss\n",
        "                ))\n",
        "                print(f'val: \\n{val_confusion_matrix}')\n",
        "\n",
        "                # Deep Copy Model if best accuracy\n",
        "                if epoch_acc > best_acc:\n",
        "                    best_acc = epoch_acc\n",
        "                    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "                # set current loss to val loss for write history\n",
        "                val_loss = epoch_loss\n",
        "\n",
        "            if phase == 'train':\n",
        "                train_acc_history.append(epoch_acc)\n",
        "                train_loss_history.append(epoch_loss)\n",
        "                train_pred_classes = np.asarray(train_pred_classes)\n",
        "                train_ground_truths = np.asarray(train_ground_truths)\n",
        "                train_accuracy, train_f1, train_precision, train_recall = get_acc_f1_precision_recall(\n",
        "                    train_pred_classes, train_ground_truths\n",
        "                )\n",
        "                train_f1_score.append(train_f1)\n",
        "                train_confusion_matrix = np.array_str(confusion_matrix(train_ground_truths, train_pred_classes, labels=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]))\n",
        "                print('Epoch: {} || Train_Acc: {} || Train_Loss: {}'.format(\n",
        "                    epoch, train_accuracy, epoch_loss\n",
        "                ))\n",
        "                print(f'train: \\n{train_confusion_matrix}')\n",
        "                plot_epoch.append(epoch)\n",
        "\n",
        "                # set current loss to train loss for write history\n",
        "                train_loss = epoch_loss\n",
        "\n",
        "        # Save Weights\n",
        "        model_name = save_weights(model, args, epoch, optimizer)\n",
        "\n",
        "        # Write History after train and validation phase\n",
        "        write_history(\n",
        "            args.history_path,\n",
        "            model_name,\n",
        "            train_loss,\n",
        "            val_loss,\n",
        "            train_accuracy,\n",
        "            val_accuracy,\n",
        "            train_f1,\n",
        "            val_f1,\n",
        "            train_precision,\n",
        "            val_precision,\n",
        "            train_recall,\n",
        "            val_recall,\n",
        "            train_confusion_matrix,\n",
        "            val_confusion_matrix\n",
        "        )\n",
        "\n",
        "    time_elapsed = time.time() - since\n",
        "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
        "    print('Best val Acc: {:4f}'.format(best_acc))\n",
        "\n",
        "    # load best model weights\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model, train_loss_history, val_loss_history, train_acc_history, val_acc_history, train_f1_score, val_f1_score, plot_epoch\n",
        "\n",
        "def check_accuracy(loader, model):\n",
        "    num_correct = 0\n",
        "    num_samples = 0\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        i = args.batch_size\n",
        "\n",
        "        pbar = tqdm(loader)\n",
        "        for sample in pbar:\n",
        "            x = sample[\"video\"].to(device=device)\n",
        "            y = sample[\"action\"].to(device=device)\n",
        "\n",
        "            scores = model(x)\n",
        "            predictions = scores.argmax(1)\n",
        "            y = y.argmax(1)\n",
        "\n",
        "            num_correct += (predictions == y).sum()\n",
        "            num_samples += predictions.size(0)\n",
        "\n",
        "            pbar.set_description('Progress: {}'.format(i/args.test_n))\n",
        "            i += args.batch_size\n",
        "\n",
        "        print(f'Got {num_correct} / {num_samples} with accuracy {float(num_correct)/float(num_samples)*100:.2f}')\n",
        "\n",
        "    model.train()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"PyTorch Version: \", torch.__version__)\n",
        "    print(\"Torchvision Version: \", torchvision.__version__)\n",
        "    print(\"Current Device: \", torch.cuda.current_device())\n",
        "    print(\"Device: \", torch.cuda.device(0))\n",
        "    print(\"Cuda Is Available: \", torch.cuda.is_available())\n",
        "    print(\"Device Count: \", torch.cuda.device_count())\n",
        "\n",
        "    # Initialize R(2+1)D Model\n",
        "    model = models.video.r2plus1d_18(pretrained=args.pretrained, progress=True)\n",
        "\n",
        "    # change final fully-connected layer to output 10 classes\n",
        "    for param in model.parameters():\n",
        "        param.requires_grad = False\n",
        "\n",
        "    # Get the number of features before modifying the fc layer\n",
        "    num_ftrs = model.fc.in_features\n",
        "\n",
        "    # Adding dropout layer before the final fully connected layer\n",
        "    dropout_rate = 0.5  # You can adjust the dropout rate here\n",
        "    model.fc = nn.Sequential(\n",
        "        nn.Dropout(dropout_rate),\n",
        "        nn.Linear(num_ftrs, args.num_classes)\n",
        "    )\n",
        "\n",
        "\n",
        "    for name, param in model.named_parameters():\n",
        "        for layer in args.layers_list:\n",
        "            if layer in name:\n",
        "                param.requires_grad = True\n",
        "    # New Model is trained with 128x176 images\n",
        "    # Calculation:\n",
        "    model.fc = nn.Linear(num_ftrs, args.num_classes, bias=True)\n",
        "    print(model)\n",
        "\n",
        "    params_to_update = model.parameters()\n",
        "    print(\"Params to learn:\")\n",
        "    params_to_update = []\n",
        "    for name, param in model.named_parameters():\n",
        "        if param.requires_grad == True:\n",
        "            params_to_update.append(param)\n",
        "            print(\"\\t\", name)\n",
        "\n",
        "    if device.type == 'cuda':\n",
        "        print(torch.cuda.get_device_name(0))\n",
        "        print('Memory Usage:')\n",
        "        print('Allocated:', round(torch.cuda.memory_allocated(0) / 1024 ** 3, 1), 'GB')\n",
        "        print('Cached:   ', round(torch.cuda.memory_reserved(0) / 1024 ** 3, 1), 'GB')\n",
        "        print(\" \")\n",
        "\n",
        "    # Transforms\n",
        "    sometimes = lambda aug: vidaug.Sometimes(0.5, aug)  # Used to apply augmentor with 50% probability\n",
        "    video_augmentation = vidaug.Sequential([\n",
        "        sometimes(vidaug.Salt()),\n",
        "        sometimes(vidaug.Pepper()),\n",
        "    ], random_order=True)\n",
        "\n",
        "    #Load Dataset\n",
        "    basketball_dataset = BasketballDataset(annotation_dict=args.annotation_path,\n",
        "                                           augmented_dict=args.augmented_annotation_path)\n",
        "\n",
        "    train_subset, test_subset = random_split(\n",
        "    basketball_dataset, [args.n_total-args.test_n, args.test_n], generator=torch.Generator().manual_seed(1))\n",
        "\n",
        "    train_subset, val_subset = random_split(\n",
        "        train_subset, [args.n_total-args.test_n-args.val_n, args.val_n], generator=torch.Generator().manual_seed(1))\n",
        "\n",
        "    train_loader = DataLoader(dataset=train_subset, shuffle=True, batch_size=args.batch_size)\n",
        "    val_loader = DataLoader(dataset=val_subset, shuffle=False, batch_size=args.batch_size)\n",
        "    test_loader = DataLoader(dataset=test_subset, shuffle=False, batch_size=args.batch_size)\n",
        "\n",
        "    dataloaders_dict = {'train': train_loader, 'val': val_loader}\n",
        "\n",
        "    # Train\n",
        "    optimizer_ft = optim.Adam(params_to_update, lr=args.lr)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    if args.continue_epoch:\n",
        "        model = load_weights(model, args)\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        # Put model into device after updating parameters\n",
        "        model = model.to(device)\n",
        "        criterion = criterion.to(device)\n",
        "\n",
        "    # Train and evaluate\n",
        "    model, train_loss_history, val_loss_history, train_acc_history, val_acc_history, train_f1_score, val_f1_score, plot_epoch = train_model(model,\n",
        "                                                                                                                                            dataloaders_dict,\n",
        "                                                                                                                                            criterion,\n",
        "                                                                                                                                            optimizer_ft,\n",
        "                                                                                                                                            args,\n",
        "                                                                                                                                            start_epoch=args.start_epoch,\n",
        "                                                                                                                                            num_epochs=args.num_epochs)\n",
        "\n",
        "        # 将数据从 GPU 转移至 CPU 并转换为 NumPy 数组，以便绘图\n",
        "    train_acc = [x.cpu().numpy() for x in train_acc_history]\n",
        "    val_acc = [x.cpu().numpy() for x in val_acc_history]\n",
        "    epochs = range(1, len(train_acc_history) + 1)\n",
        "\n",
        "    # 绘制训练和验证准确率图\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.subplot(121)\n",
        "    plt.plot(epochs, train_acc, label='Train Accuracy')\n",
        "    plt.plot(epochs, val_acc, label='Validation Accuracy')\n",
        "    plt.title('Training and Validation Accuracy')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "    print(\"Best Validation Loss: \", min(val_loss_history), \"Epoch: \", val_loss_history.index(min(val_loss_history)))\n",
        "    print(\"Best Training Loss: \", min(train_loss_history), \"Epoch: \", train_loss_history.index(min(train_loss_history)))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jnZATqrq7YWv",
        "outputId": "969acb96-5fea-4319-b5f4-71c4b4a9b575"
      },
      "outputs": [],
      "source": [
        "def check_accuracy(loader, model, device):\n",
        "    model.eval()  # Set model to evaluation mode\n",
        "\n",
        "    all_predictions = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        pbar = tqdm(loader, desc=\"Evaluating\")\n",
        "        for sample in pbar:\n",
        "            videos = sample[\"video\"].to(device)\n",
        "            labels = sample[\"action\"].to(device)\n",
        "\n",
        "            # Compute model output\n",
        "            outputs = model(videos)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "\n",
        "            # 如果标签是独热编码，需要转换它们为类别索引\n",
        "            if labels.ndim > 1 and labels.size(1) > 1:\n",
        "                labels = labels.argmax(dim=1)\n",
        "\n",
        "            # Append predictions and labels\n",
        "            all_predictions.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    # Compute metrics\n",
        "    accuracy = accuracy_score(all_labels, all_predictions)\n",
        "    precision = precision_score(all_labels, all_predictions, average='macro', zero_division=0)\n",
        "    recall = recall_score(all_labels, all_predictions, average='macro', zero_division=0)\n",
        "    f1 = f1_score(all_labels, all_predictions, average='macro', zero_division=0)\n",
        "    cm = confusion_matrix(all_labels, all_predictions)\n",
        "\n",
        "    print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
        "    print(f\"Precision: {precision:.4f}\")\n",
        "    print(f\"Recall: {recall:.4f}\")\n",
        "    print(f\"F1 Score: {f1:.4f}\")\n",
        "    print(\"Confusion Matrix:\")\n",
        "    print(cm)\n",
        "\n",
        "    model.train()  # Set model back to training mode\n",
        "\n",
        "# Usage example\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "check_accuracy(test_loader, model, device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "maf0Di-lXhUW"
      },
      "source": [
        "CONV-LSTM Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rmJy3U074qD7"
      },
      "source": [
        "資料載入"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ssi0FQp54iHb",
        "outputId": "73c3f8d0-dbe6-419e-d081-1cdb6504bb86"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "8\n",
            "8\n",
            "49899\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import numpy as np\n",
        "import cv2\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import transforms\n",
        "\n",
        "base_dir = '/content/drive/My Drive/dataset/'\n",
        "\n",
        "class VideoDataset(Dataset):\n",
        "    \"\"\"SpaceJam: a Dataset for Basketball Action Recognition.\"\"\"\n",
        "\n",
        "    def __init__(self, annotation_dict, augmented_dict, video_dir=base_dir+\"examples/\", augmented_dir=base_dir+\"augmented-examples/\", augment=True, transform=None, poseData=False):\n",
        "        with open(annotation_dict) as f:\n",
        "            self.video_list = list(json.load(f).items())\n",
        "\n",
        "        if augment:\n",
        "            self.augment = augment\n",
        "            with open(augmented_dict) as f:\n",
        "                augmented_list = list(json.load(f).items())\n",
        "            self.augmented_dir = augmented_dir\n",
        "            # extend with augmented data\n",
        "            self.video_list.extend(augmented_list)\n",
        "\n",
        "        self.video_dir = video_dir\n",
        "        self.poseData = poseData\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        # 返回视频列表的长度作为数据集的长度\n",
        "        return len(self.video_list)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "      video_id = self.video_list[idx][0]\n",
        "      encoding = int(self.video_list[idx][1])\n",
        "      video = self.VideoToNumpy(video_id)\n",
        "      if video is None:\n",
        "        return None\n",
        "      sample = {'video_id': video_id, 'video': torch.from_numpy(video).float(), 'action': encoding, 'class': encoding}\n",
        "      return sample\n",
        "\n",
        "    def VideoToNumpy(self, video_id):\n",
        "      video_path = os.path.join(self.video_dir, video_id + \".mp4\")\n",
        "      video = cv2.VideoCapture(video_path)\n",
        "      if not video.isOpened():\n",
        "        augmented_video_path = os.path.join(self.augmented_dir, video_id + \".mp4\")\n",
        "        video = cv2.VideoCapture(augmented_video_path)\n",
        "        if not video.isOpened():\n",
        "            print(f\"Cannot read video: {video_path} or {augmented_video_path}\")\n",
        "            return None\n",
        "\n",
        "      video_frames = []\n",
        "      while True:\n",
        "        success, frame = video.read()\n",
        "        if not success:\n",
        "            break\n",
        "        frame = frame.astype(np.float32) / 255.0\n",
        "        frame = np.transpose(frame, (2, 0, 1))\n",
        "        video_frames.append(frame)\n",
        "\n",
        "      video.release()\n",
        "      # 如果帧数不足16，重复最后一帧直到达到16帧\n",
        "      while len(video_frames) < 16:\n",
        "        video_frames.append(video_frames[-1])  # 假设视频至少有一帧\n",
        "\n",
        "      return np.array(video_frames)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    video_dataset = VideoDataset(annotation_dict=base_dir + \"annotation_dict.json\",\n",
        "                                 augmented_dict=base_dir + \"augmented_annotation_dict.json\")\n",
        "    print(video_dataset[1]['action'])\n",
        "    print(video_dataset[1]['class'])\n",
        "    print(len(video_dataset))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3gHNPTwe7NQf"
      },
      "source": [
        "參數設置"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Eh4MdBa37NCB"
      },
      "outputs": [],
      "source": [
        "from easydict import EasyDict\n",
        "\n",
        "base_dir = '/content/drive/My Drive/dataset/'  # 确保这个路径是正确的\n",
        "\n",
        "args = EasyDict({\n",
        "    'base_model_name': 'conv-lstm',\n",
        "    'pretrained': True,\n",
        "    'lr': 0.0001,\n",
        "    'start_epoch': 1,\n",
        "    'num_epochs': 6,\n",
        "    'layers_list': ['layer3', 'layer4', 'fc'],\n",
        "    'continue_epoch': False,\n",
        "    'num_classes': 10,\n",
        "    'batch_size': 25,\n",
        "    'n_total': 49899,\n",
        "    'test_n': 4990,\n",
        "    'val_n': 9980,\n",
        "    'annotation_path': base_dir + \"annotation_dict.json\",\n",
        "    'augmented_annotation_path': base_dir + \"augmented_annotation_dict.json\",\n",
        "    'device': 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "})\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v-ea6G9C4rqC"
      },
      "source": [
        "模型定義"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class ConvLSTMCell(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, kernel_size, bias):\n",
        "        super(ConvLSTMCell, self).__init__()\n",
        "\n",
        "        self.input_dim = input_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "\n",
        "        self.kernel_size = kernel_size\n",
        "        self.padding = kernel_size // 2\n",
        "        self.bias = bias\n",
        "\n",
        "        self.conv = nn.Conv2d(in_channels=self.input_dim + self.hidden_dim,\n",
        "                              out_channels=4 * self.hidden_dim,\n",
        "                              kernel_size=self.kernel_size,\n",
        "                              padding=self.padding,\n",
        "                              bias=self.bias)\n",
        "\n",
        "    def forward(self, input_tensor, cur_state):\n",
        "        h_cur, c_cur = cur_state\n",
        "\n",
        "        combined = torch.cat([input_tensor, h_cur], dim=1)  # concatenate along channel axis\n",
        "        combined_conv = self.conv(combined)\n",
        "        cc_i, cc_f, cc_o, cc_g = torch.split(combined_conv, self.hidden_dim, dim=1)\n",
        "        i = torch.sigmoid(cc_i)\n",
        "        f = torch.sigmoid(cc_f)\n",
        "        o = torch.sigmoid(cc_o)\n",
        "        g = torch.tanh(cc_g)\n",
        "\n",
        "        c_next = f * c_cur + i * g\n",
        "        h_next = o * torch.tanh(c_next)\n",
        "\n",
        "        return h_next, c_next\n",
        "\n",
        "    def init_hidden(self, batch_size, image_size):\n",
        "        height, width = image_size\n",
        "        return (torch.zeros(batch_size, self.hidden_dim, height, width, device=self.conv.weight.device),\n",
        "                torch.zeros(batch_size, self.hidden_dim, height, width, device=self.conv.weight.device))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sL3aRDDIoet0"
      },
      "outputs": [],
      "source": [
        "class ConvLSTM(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, kernel_size, num_layers, batch_first=False, bias=True, return_all_layers=False):\n",
        "        super(ConvLSTM, self).__init__()\n",
        "\n",
        "        self._check_kernel_size_consistency(kernel_size)\n",
        "\n",
        "        # Ensure hidden_dim and kernel_size are lists having len == num_layers\n",
        "        hidden_dim = self._extend_for_multilayer(hidden_dim, num_layers)\n",
        "        kernel_size = self._extend_for_multilayer(kernel_size, num_layers)\n",
        "        if not len(hidden_dim) == len(kernel_size) == num_layers:\n",
        "            raise ValueError('Inconsistent list length.')\n",
        "\n",
        "        self.input_dim = input_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.kernel_size = kernel_size\n",
        "        self.num_layers = num_layers\n",
        "        self.batch_first = batch_first\n",
        "        self.bias = bias\n",
        "        self.return_all_layers = return_all_layers\n",
        "\n",
        "        cell_list = []\n",
        "        for i in range(0, self.num_layers):\n",
        "            cur_input_dim = self.input_dim if i == 0 else self.hidden_dim[i - 1]\n",
        "\n",
        "            cell_list.append(ConvLSTMCell(input_dim=cur_input_dim,\n",
        "                                          hidden_dim=self.hidden_dim[i],\n",
        "                                          kernel_size=self.kernel_size[i],\n",
        "                                          bias=self.bias))\n",
        "\n",
        "        self.cell_list = nn.ModuleList(cell_list)\n",
        "\n",
        "    def forward(self, input_tensor, hidden_state=None):\n",
        "        if not self.batch_first:\n",
        "            # (time, batch, channel, height, width) -> (batch, time, channel, height, width)\n",
        "            input_tensor = input_tensor.permute(1, 0, 2, 3, 4)\n",
        "\n",
        "        batch_size, _, _, height, width = input_tensor.size()\n",
        "\n",
        "        if hidden_state is None:\n",
        "            hidden_state = self._init_hidden(batch_size=batch_size, image_size=(height, width))\n",
        "\n",
        "        layer_output_list = []\n",
        "        last_state_list = []\n",
        "\n",
        "        seq_len = input_tensor.size(1)\n",
        "        cur_layer_input = input_tensor\n",
        "\n",
        "        for layer_idx in range(self.num_layers):\n",
        "\n",
        "            h, c = hidden_state[layer_idx]\n",
        "            output_inner = []\n",
        "            for t in range(seq_len):\n",
        "                h, c = self.cell_list[layer_idx](input_tensor=cur_layer_input[:, t, :, :, :],\n",
        "                                                 cur_state=[h, c])\n",
        "                output_inner.append(h)\n",
        "\n",
        "            layer_output = torch.stack(output_inner, dim=1)\n",
        "            cur_layer_input = layer_output\n",
        "\n",
        "            layer_output_list.append(layer_output)\n",
        "            last_state_list.append([h, c])\n",
        "\n",
        "        if not self.return_all_layers:\n",
        "            layer_output_list = layer_output_list[-1:]\n",
        "            last_state_list = last_state_list[-1:]\n",
        "\n",
        "        return layer_output_list, last_state_list\n",
        "\n",
        "    def _init_hidden(self, batch_size, image_size):\n",
        "        init_states = []\n",
        "        for i in range(self.num_layers):\n",
        "            init_states.append(self.cell_list[i].init_hidden(batch_size, image_size))\n",
        "        return init_states\n",
        "\n",
        "    @staticmethod\n",
        "    def _check_kernel_size_consistency(kernel_size):\n",
        "        if not (isinstance(kernel_size, tuple) or\n",
        "                (isinstance(kernel_size, list) and all([isinstance(elem, tuple) for elem in kernel_size]))):\n",
        "            raise ValueError('`kernel_size` must be tuple or list of tuples')\n",
        "\n",
        "    @staticmethod\n",
        "    def _extend_for_multilayer(param, num_layers):\n",
        "        if not isinstance(param, list):\n",
        "            param = [param] * num_layers\n",
        "        return param\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bCjN6Jz149ca"
      },
      "source": [
        "Dataloader設置"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ks22rQ4q4__j"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader, random_split\n",
        "\n",
        "# 假设args包含所有必要的参数\n",
        "video_dataset = VideoDataset(annotation_dict=args.annotation_path, augmented_dict=args.augmented_annotation_path)\n",
        "\n",
        "# 确定训练集、验证集和测试集的大小\n",
        "train_size = 34929\n",
        "val_size = 9980\n",
        "test_size = 4990  # 测试集大小\n",
        "\n",
        "# 确保总大小与分配匹配\n",
        "total_size = train_size + val_size + test_size\n",
        "assert total_size == len(video_dataset), \"Size mismatch\"\n",
        "\n",
        "# 划分数据集\n",
        "train_val_dataset, test_dataset = random_split(video_dataset, [train_size + val_size, test_size], generator=torch.Generator().manual_seed(42))\n",
        "train_dataset, val_dataset = random_split(train_val_dataset, [train_size, val_size], generator=torch.Generator().manual_seed(42))\n",
        "\n",
        "# 创建DataLoaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=args.batch_size, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=args.batch_size, shuffle=False)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iP4UGE7n64OJ"
      },
      "source": [
        "訓練"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ktHCemOB81QN",
        "outputId": "13f506f0-a7ff-4f73-8e1c-ec1b8f02f863"
      },
      "outputs": [],
      "source": [
        "!pip install torchmetrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RSBAwP1EBCov",
        "outputId": "71cea710-a20e-45b5-8345-95ca9a844949"
      },
      "outputs": [],
      "source": [
        "!pip install tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "hfTgfSTe657P",
        "outputId": "e2495b6e-aa83-42a4-d698-88cd90748053"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.optim import Adam\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from torchmetrics import ConfusionMatrix, Accuracy, Precision, Recall, F1Score\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "base_dir = '/content/drive/My Drive/dataset/'\n",
        "# 初始化模型、优化器和损失函数\n",
        "model = ConvLSTM(num_classes=args.num_classes).to(args.device)\n",
        "optimizer = Adam(model.parameters(), lr=args.lr)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# 初始化评价指标\n",
        "metrics = {\n",
        "    'accuracy': Accuracy(num_classes=args.num_classes, average='macro', task='multiclass').to(args.device),\n",
        "    'precision': Precision(num_classes=args.num_classes, average='macro', task='multiclass').to(args.device),\n",
        "    'recall': Recall(num_classes=args.num_classes, average='macro', task='multiclass').to(args.device),\n",
        "    'f1': F1Score(num_classes=args.num_classes, average='macro', task='multiclass').to(args.device)\n",
        "}\n",
        "\n",
        "# 准备数据加载器\n",
        "train_loader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=args.batch_size, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=args.batch_size, shuffle=False)\n",
        "\n",
        "# 准备存储训练历史的文件夹和文件\n",
        "history_dir = os.path.join(base_dir+\"histories\")\n",
        "if not os.path.exists(history_dir):\n",
        "    os.makedirs(history_dir)\n",
        "history_path = os.path.join(history_dir, \"historyLSTM.txt\")\n",
        "\n",
        "# 添加精确度、召回率、F1分数和混淆矩阵到头部信息\n",
        "with open(history_path, \"w\") as f:\n",
        "    f.write(\"epoch,train_loss,train_accuracy,train_precision,train_recall,train_f1,validation_loss,validation_accuracy,validation_precision,validation_recall,validation_f1\\n\")\n",
        "\n",
        "\n",
        "def process_epoch(mode, model, loader, optimizer, criterion, device, metrics, history_file, epoch=None):\n",
        "    if mode == 'train':\n",
        "        model.train()\n",
        "    else:\n",
        "        model.eval()\n",
        "\n",
        "    total_loss = 0\n",
        "    confusion_matrix = ConfusionMatrix(num_classes=args.num_classes, task=\"multiclass\").to(device)\n",
        "    progress_bar = tqdm(loader, desc=f\"Epoch {epoch} [{mode.upper()}]\", total=len(loader))\n",
        "\n",
        "    for data in progress_bar:\n",
        "        videos = data['video'].to(device)\n",
        "        labels = data['action'].to(device)\n",
        "\n",
        "        if mode == 'train':\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "        with torch.set_grad_enabled(mode == 'train'):\n",
        "            outputs = model(videos)\n",
        "            loss = criterion(outputs, labels)\n",
        "            if mode == 'train':\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        confusion_matrix.update(predicted, labels)\n",
        "\n",
        "        for metric in metrics.values():\n",
        "            metric(outputs, labels)\n",
        "\n",
        "        progress_bar.set_postfix(loss=loss.item())\n",
        "\n",
        "    avg_loss = total_loss / len(loader)\n",
        "    metrics_results = {name: metric.compute().item() for name, metric in metrics.items()}\n",
        "    metrics_results['loss'] = avg_loss\n",
        "    cm = confusion_matrix.compute()\n",
        "\n",
        "    if history_file and epoch is not None:\n",
        "        history_file.write(f\"{epoch},{avg_loss},{metrics_results['accuracy']},{metrics_results['precision']},{metrics_results['recall']},{metrics_results['f1']}\\n\")\n",
        "\n",
        "    return metrics_results, cm\n",
        "\n",
        "#训练和验证循环调用\n",
        "with open(history_path, \"a\") as f:\n",
        "    for epoch in range(args.start_epoch, args.num_epochs + 1):\n",
        "        train_metrics, train_cm = process_epoch('train', model, train_loader, optimizer, criterion, args.device, metrics, None, epoch)\n",
        "        val_metrics, val_cm = process_epoch('validate', model, val_loader, None, criterion, args.device, metrics, None, epoch)\n",
        "\n",
        "        # 将训练和验证的结果写入文件\n",
        "        f.write(f\"{epoch},{train_metrics['loss']},{train_metrics['accuracy']},{train_metrics['precision']},{train_metrics['recall']},{train_metrics['f1']},\")\n",
        "        f.write(f\"{val_metrics['loss']},{val_metrics['accuracy']},{val_metrics['precision']},{val_metrics['recall']},{val_metrics['f1']}\\n\")\n",
        "\n",
        "        print(f\"Train CM Epoch {epoch}:\\n{train_cm}\")\n",
        "        print(f\"Validation CM Epoch {epoch}:\\n{val_cm}\")\n",
        "\n",
        "\n",
        "\n",
        "# 选项：完成所有训练后进行最终测试评估\n",
        "def evaluate(model, loader, criterion, device, metrics):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    confusion_matrix = ConfusionMatrix(num_classes=args.num_classes, task=\"multiclass\").to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for data in loader:\n",
        "            videos = data['video'].to(device)\n",
        "            labels = data['action'].to(device)\n",
        "            outputs = model(videos)\n",
        "            loss = criterion(outputs, labels)\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            confusion_matrix.update(predicted, labels)\n",
        "            for metric in metrics.values():\n",
        "                metric(outputs, labels)\n",
        "\n",
        "    avg_loss = total_loss / len(loader)\n",
        "    metrics_results = {name: metric.compute().item() for name, metric in metrics.items()}\n",
        "    metrics_results['loss'] = avg_loss\n",
        "    cm = confusion_matrix.compute()\n",
        "\n",
        "    return metrics_results, cm\n",
        "\n",
        "metrics, test_cm = evaluate(model, test_loader, criterion, args.device, metrics)\n",
        "print(f\"Final Test Metrics: {metrics}\")\n",
        "print(f\"Test Confusion Matrix:\\n{test_cm}\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xBfyGIlT2t85"
      },
      "source": [
        "Error Analysis(R(2+1)D CNN Model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 977
        },
        "id": "38DFJ883bZtv",
        "outputId": "1d19b1ec-d4cb-4051-a92f-5267e80391b5"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "# 定义你的混淆矩阵和标签\n",
        "conf_matrix = np.array([\n",
        "    [477, 6, 5, 0, 7, 3, 3, 2, 0, 6],\n",
        "    [5, 487, 3, 3, 3, 4, 0, 2, 0, 0],\n",
        "    [1, 0, 529, 4, 2, 1, 4, 2, 0, 54],\n",
        "    [0, 5, 16, 311, 0, 5, 4, 0, 0, 6],\n",
        "    [2, 4, 0, 0, 218, 1, 0, 0, 0, 0],\n",
        "    [4, 18, 8, 14, 2, 203, 2, 1, 6, 14],\n",
        "    [0, 3, 16, 3, 0, 5, 254, 4, 40, 70],\n",
        "    [2, 1, 2, 0, 0, 0, 2, 316, 2, 11],\n",
        "    [3, 4, 4, 0, 0, 2, 29, 4, 434, 138],\n",
        "    [3, 4, 69, 1, 0, 4, 27, 4, 59, 1013]\n",
        "])\n",
        "\n",
        "labels = ['block', 'pass', 'run', 'dribble', 'shoot', 'ball in hand', 'defense', 'pick', 'no_action', 'walk']\n",
        "\n",
        "# 模拟实际标签和预测标签\n",
        "y_true = []\n",
        "y_pred = []\n",
        "for i, row in enumerate(conf_matrix):\n",
        "    for j, count in enumerate(row):\n",
        "        y_true.extend([i] * count)\n",
        "        y_pred.extend([j] * count)\n",
        "\n",
        "# 生成分类报告\n",
        "report = classification_report(y_true, y_pred, target_names=labels, output_dict=True)\n",
        "\n",
        "# 计算每个类别的准确率\n",
        "accuracies = conf_matrix.diagonal() / conf_matrix.sum(axis=1)\n",
        "\n",
        "# 打印分类报告并加入每个类别的准确率\n",
        "print(f\"{'Class':<15}{'Precision':<10}{'Recall':<10}{'F1-Score':<10}{'Accuracy':<10}\")\n",
        "for label in labels:\n",
        "    precision = report[label]['precision']\n",
        "    recall = report[label]['recall']\n",
        "    f1_score = report[label]['f1-score']\n",
        "    accuracy = accuracies[labels.index(label)]\n",
        "    print(f\"{label:<15}{precision:<10.2f}{recall:<10.2f}{f1_score:<10.2f}{accuracy:<10.2f}\")\n",
        "\n",
        "# 绘制混淆矩阵\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=labels, yticklabels=labels)\n",
        "plt.xlabel('Predicted Labels')\n",
        "plt.ylabel('True Labels')\n",
        "plt.title('Confusion Matrix for R(2+1)D CNN Model on Test Set')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WflSe2xh2zlA"
      },
      "source": [
        "Error Analysis(CONV-LSTM Model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 977
        },
        "id": "dUvy38gv4WoC",
        "outputId": "8ab391a3-73b1-49a1-bafd-1495a63791c1"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "# 定义你的混淆矩阵和标签\n",
        "conf_matrix = np.array([\n",
        "    [469,   9,   4,   0,   5,   2,   7,  12,   4,   6],\n",
        "        [  5, 524,   5,   4,   4,  13,   0,   4,   2,   3],\n",
        "        [  0,   0, 450,   5,   0,   0,   3,   0,   0,  96],\n",
        "        [  0,   3,   4, 318,   0,  27,   1,   0,   0,   3],\n",
        "        [  6,   7,   0,   0, 181,   2,   0,   1,   0,   0],\n",
        "        [  3,  12,   6,   5,   1, 184,   2,   0,   1,   5],\n",
        "        [  0,   1,   7,   4,   0,   7, 252,   0,  42,  63],\n",
        "        [  1,  11,   4,   0,   0,   3,   2, 302,   5,  11],\n",
        "        [  2,   3,   1,   1,   0,   6,  22,   1, 488, 145],\n",
        "        [  2,   1,  53,   5,   0,   5,  31,   3, 108, 990]\n",
        "])\n",
        "\n",
        "labels = ['block', 'pass', 'run', 'dribble', 'shoot', 'ball in hand', 'defense', 'pick', 'no_action', 'walk']\n",
        "\n",
        "# 模拟实际标签和预测标签\n",
        "y_true = []\n",
        "y_pred = []\n",
        "for i, row in enumerate(conf_matrix):\n",
        "    for j, count in enumerate(row):\n",
        "        y_true.extend([i] * count)\n",
        "        y_pred.extend([j] * count)\n",
        "\n",
        "# 生成分类报告\n",
        "report = classification_report(y_true, y_pred, target_names=labels, output_dict=True)\n",
        "\n",
        "# 计算每个类别的准确率\n",
        "accuracies = conf_matrix.diagonal() / conf_matrix.sum(axis=1)\n",
        "\n",
        "# 打印分类报告并加入每个类别的准确率\n",
        "print(f\"{'Class':<15}{'Precision':<10}{'Recall':<10}{'F1-Score':<10}{'Accuracy':<10}\")\n",
        "for label in labels:\n",
        "    precision = report[label]['precision']\n",
        "    recall = report[label]['recall']\n",
        "    f1_score = report[label]['f1-score']\n",
        "    accuracy = accuracies[labels.index(label)]\n",
        "    print(f\"{label:<15}{precision:<10.2f}{recall:<10.2f}{f1_score:<10.2f}{accuracy:<10.2f}\")\n",
        "\n",
        "# 绘制混淆矩阵\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=labels, yticklabels=labels)\n",
        "plt.xlabel('Predicted Labels')\n",
        "plt.ylabel('True Labels')\n",
        "plt.title('Confusion Matrix for CONV-LSTM Model on Test Set')\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
